{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission\n",
    "\n",
    "* Write a text classification pipeline using a custom preprocessor and CharNGramAnalyzer using data from Wikipedia articles as training set.\n",
    "\n",
    "* Evaluate the performance on some held out test set.\n",
    "\n",
    "* [solution](https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "985\n",
      "['ar', 'de', 'en', 'es', 'fr', 'it', 'ja', 'nl', 'pl', 'pt', 'ru']\n",
      "8805\n",
      "['ar', 'de', 'en', 'es', 'fr', 'it', 'ja', 'nl', 'pl', 'pt', 'ru']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "training_set_path = \"./data/languages/paragraphs\"\n",
    "test_set_paht = \"./data/languages/short_paragraphs\"\n",
    "training_set = load_files( training_set_path )\n",
    "test_set = load_files( test_set_paht )\n",
    "\n",
    "print( len( training_set.data ) )\n",
    "print( training_set.target_names)\n",
    "\n",
    "print( len( test_set.data ) )\n",
    "print( test_set.target_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='char', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "## Naive Bayes Classicifier\n",
    "text_clf_nb = Pipeline([\n",
    "    ( 'vect', CountVectorizer(analyzer='char', decode_error='ignore') ),\n",
    "    ( 'tfidf', TfidfTransformer() ),\n",
    "    ( 'clf', MultinomialNB() )\n",
    "])\n",
    "\n",
    "text_clf_nb.fit( training_set.data, training_set.target )\n",
    "\n",
    "## SVM Classicifier\n",
    "text_clf_svm = Pipeline([\n",
    "    ( 'vect', CountVectorizer(analyzer='char', decode_error='ignore') ),\n",
    "    ( 'tfidf', TfidfTransformer() ),\n",
    "    ( 'clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None) )\n",
    "])\n",
    "\n",
    "text_clf_svm.fit( training_set.data, training_set.target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49074389551391256\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         ar       1.00      1.00      1.00       332\n",
      "         de       0.34      0.94      0.50      1075\n",
      "         en       0.28      0.77      0.42      1085\n",
      "         es       0.75      0.34      0.47      1056\n",
      "         fr       0.73      0.62      0.67      1054\n",
      "         it       0.00      0.00      0.00      1019\n",
      "         ja       0.00      0.00      0.00       580\n",
      "         nl       1.00      0.02      0.04       590\n",
      "         pl       1.00      0.17      0.28      1056\n",
      "         pt       1.00      0.98      0.99       958\n",
      "\n",
      "avg / total       0.59      0.49      0.43      8805\n",
      "\n",
      "0.705167518455423\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         ar       1.00      1.00      1.00       332\n",
      "         de       0.66      0.79      0.72      1075\n",
      "         en       0.65      0.51      0.57      1085\n",
      "         es       0.58      0.62      0.60      1056\n",
      "         fr       0.79      0.68      0.73      1054\n",
      "         it       0.56      0.69      0.62      1019\n",
      "         ja       0.00      0.00      0.00         0\n",
      "         nl       0.56      0.39      0.46       580\n",
      "         pl       0.98      0.77      0.86       590\n",
      "         pt       0.63      0.73      0.67      1056\n",
      "         ru       1.00      0.99      0.99       958\n",
      "\n",
      "avg / total       0.71      0.71      0.70      8805\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 10, does not match size of target_names, 11\n",
      "  .format(len(labels), len(target_names))\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def report( clf, test_set ):\n",
    "    predicted = clf.predict( test_set.data )\n",
    "    rst1 = np.mean( predicted == test_set.target )\n",
    "    print( rst1 )\n",
    "    rst2 = metrics.classification_report( test_set.target, predicted, target_names=test_set.target_names )\n",
    "    print( rst2 )\n",
    "\n",
    "report( text_clf_nb, test_set )\n",
    "report( text_clf_svm, test_set )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9979687056943263\n",
      "clf__alpha: 0.001\n",
      "clf__max_iter: 10\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1,1), (1,2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "    'clf__max_iter': (5, 10),\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV( text_clf_svm, parameters, cv=5, iid=False, n_jobs=-1 )\n",
    "\n",
    "gs_clf = gs_clf.fit( training_set.data, training_set.target )\n",
    "\n",
    "print( gs_clf.best_score_ )\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
